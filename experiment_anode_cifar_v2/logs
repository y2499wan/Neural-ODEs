/home/yjyjyyj/Desktop/node/sonode/experiments/mnist/mnist_anode.py
import os
import argparse
import logging
import time
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import torchvision.datasets as datasets
import torchvision.transforms as transforms
from models import conv1x1, Flatten, conv3x3, ConcatConv2d
from dataloaders import get_cifar_loaders, get_mnist_loaders
from utils import get_logger, makedirs, count_parameters, inf_generator, learning_rate_with_decay, RunningAverageMeter, accuracy

parser = argparse.ArgumentParser()
parser.add_argument('--network', type=str, choices=['resnet', 'odenet'], default='odenet')
parser.add_argument('--tol', type=float, default=1e-3)
parser.add_argument('--adjoint', type=eval, default=False, choices=[True, False])
parser.add_argument('--downsampling-method', type=str, default='conv', choices=['conv', 'res'])
parser.add_argument('--nepochs', type=int, default=120)
parser.add_argument('--data_aug', type=eval, default=True, choices=[True, False])
parser.add_argument('--lr', type=float, default=0.1)
parser.add_argument('--batch_size', type=int, default=128)
parser.add_argument('--test_batch_size', type=int, default=1000)
parser.add_argument('--extra_channels', type=int, default=1)
parser.add_argument('--dataset', type=str, choices=['mnist', 'cifar'], default='cifar')
parser.add_argument('--save', type=str, default='./experiment_anode1')
parser.add_argument('--debug', action='store_true')
parser.add_argument('--gpu', type=int, default=0)
args = parser.parse_args()

if args.adjoint:
    from torchdiffeq import odeint_adjoint as odeint
else:
    from torchdiffeq import odeint

# norm is different from the other two methods (SONODE and NODE)
def norm(dim):
    return nn.GroupNorm(dim, dim)


class ResBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(ResBlock, self).__init__()
        self.norm1 = norm(inplanes)
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.norm2 = norm(planes)
        self.conv2 = conv3x3(planes, planes)

    def forward(self, x):
        shortcut = x

        out = self.relu(self.norm1(x))

        if self.downsample is not None:
            shortcut = self.downsample(out)

        out = self.conv1(out)
        out = self.norm2(out)
        out = self.relu(out)
        out = self.conv2(out)

        return out + shortcut

class ODEfunc(nn.Module):

    def __init__(self, dim):
        super(ODEfunc, self).__init__()
        self.norm1 = norm(dim)
        self.relu = nn.ReLU(inplace=True)
        self.conv1 = ConcatConv2d(dim, dim, 3, 1, 1)
        self.norm2 = norm(dim)
        self.conv2 = ConcatConv2d(dim, dim, 3, 1, 1)
        self.norm3 = norm(dim)
        self.nfe = 0

    def forward(self, t, x):
        self.nfe += 1
        out = self.norm1(x)
        out = self.relu(out)
        out = self.conv1(t, out)
        out = self.norm2(out)
        out = self.relu(out)
        out = self.conv2(t, out)
        out = self.norm3(out)
        return out


class ODEBlock(nn.Module):

    def __init__(self, odefunc):
        super(ODEBlock, self).__init__()
        self.odefunc = odefunc
        self.integration_time = torch.tensor([0, 1]).float()
        # self.zeros = torch.zeros(args.extra_channels, 7, 7).float().to(device)

    def forward(self, x):
        sz = x[0].size()[-1]
        zeros = torch.zeros(args.extra_channels, sz, sz).float().to(device)
        z = torch.empty(len(x), 64+args.extra_channels, sz, sz).to(device)
        for i in range(len(x)):
            z[i] = torch.cat((x[i], zeros))
        self.integration_time = self.integration_time.type_as(z)
        out = odeint(self.odefunc, z, self.integration_time, rtol=args.tol, atol=args.tol)
        out = out[1]
        return out

    @property
    def nfe(self):
        return self.odefunc.nfe

    @nfe.setter
    def nfe(self, value):
        self.odefunc.nfe = value


if __name__ == '__main__':

    makedirs(args.save)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))
    logger.info(args)

    device = torch.device('cuda:' + str(args.gpu) if torch.cuda.is_available() else 'cpu')

    is_odenet = args.network == 'odenet'
    in_channel = 3
    if args.dataset == 'mnist':
        in_channel = 1
    if args.downsampling_method == 'conv':
        downsampling_layers = [
            nn.Conv2d(in_channel, 64, 3, 1),
            norm(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 4, 2, 1),
            norm(64),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 64, 4, 2, 1),
        ]
    elif args.downsampling_method == 'res':
        downsampling_layers = [
            nn.Conv2d(in_channel, 64, 3, 1),
            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),
            ResBlock(64, 64, stride=2, downsample=conv1x1(64, 64, 2)),
        ]

    feature_layers = [ODEBlock(ODEfunc(64+args.extra_channels))] if is_odenet else [ResBlock(64, 64) for _ in range(6)]
    fc_layers = [norm(64+args.extra_channels), nn.ReLU(inplace=True), nn.AdaptiveAvgPool2d((1, 1)), Flatten(), nn.Linear(64+args.extra_channels, 10)]

    model = nn.Sequential(*downsampling_layers, *feature_layers, *fc_layers).to(device)

    logger.info(model)
    logger.info('Number of parameters: {}'.format(count_parameters(model)))

    criterion = nn.CrossEntropyLoss().to(device)

    if args.dataset == 'mnist':
        train_loader, test_loader, train_eval_loader = get_mnist_loaders(
            args.data_aug, args.batch_size, args.test_batch_size
        )
    elif args.dataset == 'cifar':
        train_loader, test_loader, train_eval_loader = get_cifar_loaders(
            args.data_aug, args.batch_size, args.test_batch_size
        )

    data_gen = inf_generator(train_loader)
    batches_per_epoch = len(train_loader)

    lr_fn = learning_rate_with_decay(
        args.batch_size, batch_denom=128, batches_per_epoch=batches_per_epoch, boundary_epochs=[60, 100, 140],
        decay_rates=[1, 0.1, 0.01, 0.001], lr=args.lr
    )

    optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, momentum=0.9)

    best_acc = 0
    batch_time_meter = RunningAverageMeter()
    f_nfe_meter = RunningAverageMeter()
    b_nfe_meter = RunningAverageMeter()
    end = time.time()
    
    epoch_arr = []
    time_val_arr = []
    time_avg_arr = []
    nfe_f_arr = []
    nfe_b_arr = []
    train_acc_arr = []
    test_acc_arr = []
    
    
    for itr in range(args.nepochs * batches_per_epoch):

        for param_group in optimizer.param_groups:
            param_group['lr'] = lr_fn(itr)

        optimizer.zero_grad()
        x, y = data_gen.__next__()
        x = x.to(device)
        y = y.to(device)
        logits = model(x)
        loss = criterion(logits, y)

        if is_odenet:
            nfe_forward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        loss.backward()
        optimizer.step()

        if is_odenet:
            nfe_backward = feature_layers[0].nfe
            feature_layers[0].nfe = 0

        batch_time_meter.update(time.time() - end)
        if is_odenet:
            f_nfe_meter.update(nfe_forward)
            b_nfe_meter.update(nfe_backward)
        end = time.time()

        if itr % batches_per_epoch == 0:
            with torch.no_grad():
                train_acc = accuracy(model, train_eval_loader, device)
                val_acc = accuracy(model, test_loader, device)
                if val_acc > best_acc:
                    torch.save({'state_dict': model.state_dict(), 'args': args}, os.path.join(args.save, 'model.pth'))
                    best_acc = val_acc
                logger.info(
                    "Epoch {:04d} | Time {:.3f} ({:.3f}) | NFE-F {:.1f} | NFE-B {:.1f} | "
                    "Train Acc {:.4f} | Test Acc {:.4f}".format(
                        itr // batches_per_epoch, batch_time_meter.val, batch_time_meter.avg, f_nfe_meter.avg,
                        b_nfe_meter.avg, train_acc, val_acc
                    )
                )
                epoch_arr += [itr // batches_per_epoch]
                time_val_arr += [batch_time_meter.val]
                time_avg_arr += [batch_time_meter.avg]
                nfe_f_arr += [f_nfe_meter.avg]
                nfe_b_arr += [b_nfe_meter.avg]
                train_acc_arr += [train_acc]
                test_acc_arr += [val_acc]
    
    epoch_arr = np.asarray(epoch_arr)
    time_val_arr = np.asarray(time_val_arr)
    time_avg_arr = np.asarray(time_avg_arr)
    nfe_f_arr = np.asarray(nfe_f_arr)
    nfe_b_arr = np.asarray(nfe_b_arr)
    train_acc_arr = np.asarray(train_acc_arr)
    test_acc_arr = np.asarray(test_acc_arr)
    
    np.save(os.path.join(args.save, 'epoch_arr.npy'), epoch_arr)
    np.save(os.path.join(args.save, 'time_val_arr.npy'), time_val_arr)
    np.save(os.path.join(args.save, 'time_avg_arr.npy'), time_avg_arr)
    np.save(os.path.join(args.save, 'nfe_f_arr.npy'), nfe_f_arr)
    np.save(os.path.join(args.save, 'nfe_b_arr.npy'), nfe_b_arr)
    np.save(os.path.join(args.save, 'train_acc_arr.npy'), train_acc_arr)
    np.save(os.path.join(args.save, 'test_acc_arr.npy'), test_acc_arr)
    
    
    
    
Namespace(adjoint=False, batch_size=128, data_aug=True, dataset='cifar', debug=False, downsampling_method='conv', extra_channels=1, gpu=0, lr=0.1, nepochs=50, network='odenet', save='./experiment_anode_v2', test_batch_size=1000, tol=0.001)
Sequential(
  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))
  (1): GroupNorm(64, 64, eps=1e-05, affine=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (4): GroupNorm(64, 64, eps=1e-05, affine=True)
  (5): ReLU(inplace=True)
  (6): Conv2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
  (7): ODEBlock(
    (odefunc): ODEfunc(
      (norm1): GroupNorm(65, 65, eps=1e-05, affine=True)
      (relu): ReLU(inplace=True)
      (conv1): ConcatConv2d(
        (_layer): Conv2d(66, 65, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm2): GroupNorm(65, 65, eps=1e-05, affine=True)
      (conv2): ConcatConv2d(
        (_layer): Conv2d(66, 65, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (norm3): GroupNorm(65, 65, eps=1e-05, affine=True)
    )
  )
  (8): GroupNorm(65, 65, eps=1e-05, affine=True)
  (9): ReLU(inplace=True)
  (10): AdaptiveAvgPool2d(output_size=(1, 1))
  (11): Flatten()
  (12): Linear(in_features=65, out_features=10, bias=True)
)
Number of parameters: 211778
Epoch 0000 | Time 1.221 (1.221) | NFE-F 32.0 | NFE-B 0.0 | Train Acc 0.0976 | Test Acc 0.0960
Epoch 0001 | Time 0.269 (0.193) | NFE-F 26.8 | NFE-B 0.0 | Train Acc 0.2916 | Test Acc 0.2882
Epoch 0002 | Time 0.322 (0.193) | NFE-F 31.4 | NFE-B 0.0 | Train Acc 0.4104 | Test Acc 0.4032
Epoch 0003 | Time 0.311 (0.197) | NFE-F 32.2 | NFE-B 0.0 | Train Acc 0.5059 | Test Acc 0.4957
Epoch 0004 | Time 0.308 (0.198) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5406 | Test Acc 0.5290
Epoch 0005 | Time 0.312 (0.198) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.5876 | Test Acc 0.5810
Epoch 0006 | Time 0.316 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6495 | Test Acc 0.6359
Epoch 0007 | Time 0.304 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6801 | Test Acc 0.6563
Epoch 0008 | Time 0.311 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6941 | Test Acc 0.6733
Epoch 0009 | Time 0.304 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.6951 | Test Acc 0.6715
Epoch 0010 | Time 0.310 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7029 | Test Acc 0.6694
Epoch 0011 | Time 0.306 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7167 | Test Acc 0.6874
Epoch 0012 | Time 0.306 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7451 | Test Acc 0.7133
Epoch 0013 | Time 0.310 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7316 | Test Acc 0.6978
Epoch 0014 | Time 0.302 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7455 | Test Acc 0.7116
Epoch 0015 | Time 0.302 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7691 | Test Acc 0.7315
Epoch 0016 | Time 0.308 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7586 | Test Acc 0.7147
Epoch 0017 | Time 0.311 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7824 | Test Acc 0.7394
Epoch 0018 | Time 0.311 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7885 | Test Acc 0.7434
Epoch 0019 | Time 0.322 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7852 | Test Acc 0.7419
Epoch 0020 | Time 0.308 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7977 | Test Acc 0.7509
Epoch 0021 | Time 0.309 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7931 | Test Acc 0.7442
Epoch 0022 | Time 0.305 (0.199) | NFE-F 32.5 | NFE-B 0.0 | Train Acc 0.7969 | Test Acc 0.7412
Epoch 0023 | Time 0.305 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.8108 | Test Acc 0.7561
Epoch 0024 | Time 0.309 (0.199) | NFE-F 32.4 | NFE-B 0.0 | Train Acc 0.7982 | Test Acc 0.7486
Epoch 0025 | Time 0.318 (0.202) | NFE-F 32.9 | NFE-B 0.0 | Train Acc 0.7867 | Test Acc 0.7354
Epoch 0026 | Time 0.311 (0.204) | NFE-F 33.3 | NFE-B 0.0 | Train Acc 0.8206 | Test Acc 0.7660
Epoch 0027 | Time 0.309 (0.208) | NFE-F 34.1 | NFE-B 0.0 | Train Acc 0.8048 | Test Acc 0.7502
Epoch 0028 | Time 0.311 (0.207) | NFE-F 34.0 | NFE-B 0.0 | Train Acc 0.8241 | Test Acc 0.7629
Epoch 0029 | Time 0.300 (0.209) | NFE-F 34.5 | NFE-B 0.0 | Train Acc 0.8233 | Test Acc 0.7677
Epoch 0030 | Time 0.328 (0.216) | NFE-F 36.0 | NFE-B 0.0 | Train Acc 0.8249 | Test Acc 0.7696
Epoch 0031 | Time 0.330 (0.223) | NFE-F 37.3 | NFE-B 0.0 | Train Acc 0.8370 | Test Acc 0.7742
Epoch 0032 | Time 0.332 (0.226) | NFE-F 37.9 | NFE-B 0.0 | Train Acc 0.8286 | Test Acc 0.7669
Epoch 0033 | Time 0.328 (0.228) | NFE-F 38.2 | NFE-B 0.0 | Train Acc 0.8313 | Test Acc 0.7682
Epoch 0034 | Time 0.334 (0.226) | NFE-F 37.1 | NFE-B 0.0 | Train Acc 0.8159 | Test Acc 0.7591
Epoch 0035 | Time 0.349 (0.232) | NFE-F 38.4 | NFE-B 0.0 | Train Acc 0.8448 | Test Acc 0.7762
Epoch 0036 | Time 0.360 (0.233) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8477 | Test Acc 0.7793
Epoch 0037 | Time 0.349 (0.232) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8473 | Test Acc 0.7790
Epoch 0038 | Time 0.339 (0.232) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8435 | Test Acc 0.7757
Epoch 0039 | Time 0.347 (0.231) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8501 | Test Acc 0.7804
Epoch 0040 | Time 0.363 (0.232) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8582 | Test Acc 0.7845
Epoch 0041 | Time 0.367 (0.231) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8513 | Test Acc 0.7769
Epoch 0042 | Time 0.338 (0.230) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8514 | Test Acc 0.7787
Epoch 0043 | Time 0.338 (0.231) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8582 | Test Acc 0.7913
Epoch 0044 | Time 0.352 (0.231) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8533 | Test Acc 0.7823
Epoch 0045 | Time 0.345 (0.230) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8617 | Test Acc 0.7873
Epoch 0046 | Time 0.336 (0.231) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8655 | Test Acc 0.7911
Epoch 0047 | Time 0.337 (0.230) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8725 | Test Acc 0.7938
Epoch 0048 | Time 0.340 (0.231) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8714 | Test Acc 0.7949
Epoch 0049 | Time 0.344 (0.230) | NFE-F 38.5 | NFE-B 0.0 | Train Acc 0.8803 | Test Acc 0.7976
